{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAFA6 Protein Function Prediction - Plan 1: Initial Approach\n",
    "\n",
    "This notebook implements a solution for the CAFA6 challenge using ESM2 protein embeddings, GO term embeddings, and cosine similarity for associations.\n",
    "\n",
    "- **ESM2 Model**: facebook/esm2_t6_8M_UR50D\n",
    "- **GO Embeddings**: Simple graph-based embeddings\n",
    "- **Association**: Cosine similarity with thresholding and ontology propagation\n",
    "\n",
    "Data path: `/kaggle/input/cafa-6-protein-function-prediction/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install any missing packages if needed\n",
    "# !pip install transformers torch networkx obonet biopython\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import networkx as nx\n",
    "import obonet\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Data paths\n",
    "data_dir = '/kaggle/input/cafa-6-protein-function-prediction/'\n",
    "train_seq_file = os.path.join(data_dir, 'train_sequences.fasta')\n",
    "train_terms_file = os.path.join(data_dir, 'train_terms.tsv')\n",
    "go_obo_file = os.path.join(data_dir, 'go-basic.obo')\n",
    "ia_file = os.path.join(data_dir, 'IA.tsv')\n",
    "test_seq_file = os.path.join(data_dir, 'testsuperset.fasta')\n",
    "\n",
    "print('Environment setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Preprocess Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training sequences\n",
    "train_sequences = {}\n",
    "for record in SeqIO.parse(train_seq_file, 'fasta'):\n",
    "    train_sequences[record.id] = str(record.seq)\n",
    "print(f'Loaded {len(train_sequences)} training sequences')\n",
    "\n",
    "# Load training terms\n",
    "train_terms = pd.read_csv(train_terms_file, sep='\\t', header=None, names=['EntryID', 'term', 'aspect'])\n",
    "print(f'Loaded {len(train_terms)} training term annotations')\n",
    "print(train_terms.head())\n",
    "\n",
    "# Load IA weights\n",
    "ia_weights = pd.read_csv(ia_file, sep='\\t', header=None, names=['term', 'weight'])\n",
    "ia_dict = dict(zip(ia_weights['term'], ia_weights['weight']))\n",
    "print(f'Loaded IA weights for {len(ia_dict)} terms')\n",
    "\n",
    "# Load GO ontology\n",
    "go_graph = obonet.read_obo(go_obo_file)\n",
    "print(f'Loaded GO graph with {len(go_graph.nodes)} nodes and {len(go_graph.edges)} edges')\n",
    "\n",
    "# Get unique GO terms from training\n",
    "go_terms = train_terms['term'].unique()\n",
    "print(f'Unique GO terms in training: {len(go_terms)}')\n",
    "\n",
    "# Subset GO graph to relevant terms\n",
    "relevant_nodes = set(go_terms)\n",
    "for term in go_terms:\n",
    "    if term in go_graph:\n",
    "        ancestors = nx.ancestors(go_graph, term)\n",
    "        relevant_nodes.update(ancestors)\n",
    "go_subgraph = go_graph.subgraph(relevant_nodes)\n",
    "print(f'Relevant GO subgraph: {len(go_subgraph.nodes)} nodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compute ESM2 Protein Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ESM2 model and tokenizer\n",
    "model_name = 'facebook/esm2_t6_8M_UR50D'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f'Loaded ESM2 model: {model_name}')\n",
    "\n",
    "# Function to get embeddings\n",
    "def get_esm_embedding(sequence):\n",
    "    inputs = tokenizer(sequence, return_tensors='pt', truncation=True, max_length=1024).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use mean pooling over sequence length\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return embedding.flatten()\n",
    "\n",
    "# Compute embeddings for training proteins (batch to save memory)\n",
    "batch_size = 10  # Adjust based on GPU memory\n",
    "protein_ids = list(train_sequences.keys())\n",
    "protein_embeddings = {}\n",
    "\n",
    "for i in range(0, len(protein_ids), batch_size):\n",
    "    batch_ids = protein_ids[i:i+batch_size]\n",
    "    batch_seqs = [train_sequences[pid] for pid in batch_ids]\n",
    "    \n",
    "    # Tokenize batch\n",
    "    inputs = tokenizer(batch_seqs, return_tensors='pt', truncation=True, max_length=1024, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Mean pooling\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    \n",
    "    for j, pid in enumerate(batch_ids):\n",
    "        protein_embeddings[pid] = embeddings[j]\n",
    "    \n",
    "    if (i // batch_size) % 10 == 0:\n",
    "        print(f'Processed {i + len(batch_ids)} / {len(protein_ids)} proteins')\n",
    "\n",
    "print(f'Computed embeddings for {len(protein_embeddings)} proteins, embedding dim: {protein_embeddings[protein_ids[0]].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compute GO Term Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple GO embedding using graph features: degree, depth, and random projection\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Compute features for each term\n",
    "go_features = {}\n",
    "for node in go_subgraph.nodes:\n",
    "    degree = go_subgraph.degree(node)\n",
    "    # Depth: distance from root (approximate)\n",
    "    try:\n",
    "        depth = nx.shortest_path_length(go_subgraph, source=list(go_subgraph.nodes)[0], target=node)  # Assuming first node is root-like\n",
    "    except:\n",
    "        depth = 0\n",
    "    go_features[node] = [degree, depth]\n",
    "\n",
    "# Convert to array\n",
    "go_terms_list = list(go_features.keys())\n",
    "features_array = np.array([go_features[term] for term in go_terms_list])\n",
    "\n",
    "# Standardize and reduce to embedding dim (match protein dim ~320)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_array)\n",
    "pca = PCA(n_components=320)\n",
    "go_embeddings = pca.fit_transform(features_scaled)\n",
    "\n",
    "# Dict for quick lookup\n",
    "go_embeddings_dict = {term: go_embeddings[i] for i, term in enumerate(go_terms_list)}\n",
    "\n",
    "print(f'Computed embeddings for {len(go_embeddings_dict)} GO terms, embedding dim: {go_embeddings.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Develop Similarity-Based Association Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict GO terms for a protein using cosine similarity\n",
    "def predict_go_terms(protein_emb, go_embeddings_dict, threshold=0.5):\n",
    "    predictions = {}\n",
    "    for term, go_emb in go_embeddings_dict.items():\n",
    "        sim = cosine_similarity([protein_emb], [go_emb])[0][0]\n",
    "        if sim > threshold:\n",
    "            predictions[term] = sim\n",
    "    return predictions\n",
    "\n",
    "# Test on a small subset for validation\n",
    "sample_proteins = list(protein_embeddings.keys())[:5]\n",
    "for pid in sample_proteins:\n",
    "    preds = predict_go_terms(protein_embeddings[pid], go_embeddings_dict, threshold=0.7)\n",
    "    print(f'Protein {pid}: {len(preds)} predicted terms')\n",
    "\n",
    "# Note: Full prediction for all proteins/terms is computationally expensive; optimize for test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Implement Ontology Propagation for Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to propagate predictions to ancestors\n",
    "def propagate_predictions(predictions, go_graph):\n",
    "    propagated = predictions.copy()\n",
    "    for term, score in predictions.items():\n",
    "        if term in go_graph:\n",
    "            ancestors = nx.ancestors(go_graph, term)\n",
    "            for anc in ancestors:\n",
    "                if anc not in propagated or propagated[anc] < score:\n",
    "                    propagated[anc] = score\n",
    "    return propagated\n",
    "\n",
    "# Test propagation on sample\n",
    "sample_preds = predict_go_terms(protein_embeddings[sample_proteins[0]], go_embeddings_dict, threshold=0.7)\n",
    "propagated_preds = propagate_predictions(sample_preds, go_subgraph)\n",
    "print(f'Original: {len(sample_preds)}, Propagated: {len(propagated_preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train and Validate Model on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data for validation\n",
    "train_proteins, val_proteins = train_test_split(list(protein_embeddings.keys()), test_size=0.2, random_state=42)\n",
    "print(f'Train: {len(train_proteins)}, Val: {len(val_proteins)}')\n",
    "\n",
    "# Tune threshold on validation set (simplified: use fixed threshold for now)\n",
    "threshold = 0.5  # Tune this based on validation performance\n",
    "\n",
    "# Compute predictions for validation proteins\n",
    "val_predictions = {}\n",
    "for pid in val_proteins[:10]:  # Subset for speed\n",
    "    preds = predict_go_terms(protein_embeddings[pid], go_embeddings_dict, threshold)\n",
    "    propagated = propagate_predictions(preds, go_subgraph)\n",
    "    val_predictions[pid] = propagated\n",
    "\n",
    "print(f'Validation predictions computed for {len(val_predictions)} proteins')\n",
    "\n",
    "# Note: Implement full F1 evaluation here if time allows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Generate Predictions for Test Superset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test sequences\n",
    "test_sequences = {}\n",
    "for record in SeqIO.parse(test_seq_file, 'fasta'):\n",
    "    test_sequences[record.id] = str(record.seq)\n",
    "print(f'Loaded {len(test_sequences)} test sequences')\n",
    "\n",
    "# Compute embeddings for test proteins\n",
    "test_embeddings = {}\n",
    "test_ids = list(test_sequences.keys())\n",
    "for i in range(0, len(test_ids), batch_size):\n",
    "    batch_ids = test_ids[i:i+batch_size]\n",
    "    batch_seqs = [test_sequences[pid] for pid in batch_ids]\n",
    "    \n",
    "    inputs = tokenizer(batch_seqs, return_tensors='pt', truncation=True, max_length=1024, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    \n",
    "    for j, pid in enumerate(batch_ids):\n",
    "        test_embeddings[pid] = embeddings[j]\n",
    "    \n",
    "    if (i // batch_size) % 10 == 0:\n",
    "        print(f'Processed {i + len(batch_ids)} / {len(test_ids)} test proteins')\n",
    "\n",
    "# Generate predictions for test set\n",
    "test_predictions = {}\n",
    "for pid in test_ids:\n",
    "    preds = predict_go_terms(test_embeddings[pid], go_embeddings_dict, threshold)\n",
    "    propagated = propagate_predictions(preds, go_subgraph)\n",
    "    test_predictions[pid] = propagated\n",
    "\n",
    "print(f'Generated predictions for {len(test_predictions)} test proteins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Prepare and Output Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare submission dataframe\n",
    "submission_rows = []\n",
    "for pid, preds in test_predictions.items():\n",
    "    for term, score in preds.items():\n",
    "        if score > 0:  # Only include positive predictions\n",
    "            submission_rows.append([pid, term, score])\n",
    "\n",
    "submission_df = pd.DataFrame(submission_rows, columns=['Protein ID', 'GO Term', 'Score'])\n",
    "submission_df = submission_df.sort_values(['Protein ID', 'GO Term'])\n",
    "\n",
    "# Save to file\n",
    "submission_df.to_csv('/kaggle/working/submission.tsv', sep='\\t', index=False, header=False)\n",
    "print('Submission file saved to /kaggle/working/submission.tsv')\n",
    "print(f'Total predictions: {len(submission_df)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}